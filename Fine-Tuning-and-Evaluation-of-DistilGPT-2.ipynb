{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxJn069QLn8O"
      },
      "source": [
        "# SIT744 Assignment 4 (T2 2025)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quGPI8sl6mQP"
      },
      "source": [
        "## P-Level Tasks (~60%)\n",
        "\n",
        "Goal: Evaluate Pretrained Model’s Generative Baseline\n",
        "\n",
        "1. Model & Domain Selection\n",
        "\n",
        "    . Choose a small, widely accessible GPT‑style model (e.g., GPT‑2 small or DistilGPT‑2).\n",
        "\n",
        "    . Pick a distinctive writing domain or author (e.g., Jane Austen’s novels, satirical news articles, scientific abstracts).\n",
        "\n",
        "i will chose DistilGPT-2 due to faster, smaller model with fewer computational resources\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "model_id = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogDzFrHn_Xk8",
        "outputId": "f26d8714-03aa-4785-8c27-df08bd0d5c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Data Preparation\n",
        "\n",
        "    . Collect ~5–10 representative samples from your chosen domain.\n",
        "    \n",
        "    . Keep them short and manageable (e.g., one paragraph each).\n",
        "\n",
        "My domain sample is scientific abstract that is related to machine learning in healthcare.\n"
      ],
      "metadata": {
        "id": "8OlABhdm_cv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain_sample = [\n",
        "    \"Machine learning (ML) and its applications in healthcare have gained a lot of attention. When enhanced computational power is combined with big data, there is an opportunity to use ML algorithms to improve health care. Supervised learning is the type of ML that can be implemented to predict labeled data based on algorithms such as linear or logistic regression, support vector machine, decision tree, LASSO regression, K Nearest Neighbor, and Naive Bayes classifier. Unsupervised ML models can identify data patterns in datasets that do not contain information about the outcome. Such models can be used for fraud or anomaly detection. Examples of clinical applications of ML include the formulation of various clinical decision support systems. An important public health application of ML is the identification and prediction of populations at high risk for developing certain adverse health outcomes and the development of public health interventions targeted to these populations. Various concepts related to ML need to be integrated into the medical curriculum so that health professionals can effectively guide and interpret research in this area.\",\n",
        "    \"Machine learning (ML) is a study of computer algorithms for automation through experience. ML is a subset of artificial intelligence (AI) that develops computer systems, which are able to perform tasks generally having need of human intelligence. While healthcare communication is important in order to tactfully translate and disseminate information to support and educate patients and public, ML is proven applicable in healthcare with the ability for complex dialogue management and conversational flexibility. In this topical review, we will highlight how the application of ML/AI in healthcare communication is able to benefit humans. This includes chatbots for the COVID-19 health education, cancer therapy, and medical imaging. \",\n",
        "    \"The increasing availability of electronic health data presents a major opportunity in healthcare for both discovery and practical applications to improve healthcare. However, for healthcare epidemiologists to best use these data, computational techniques that can handle large complex datasets are required. Machine learning (ML), the study of tools and methods for identifying patterns in data, can help. The appropriate application of ML to these data promises to transform patient risk stratification broadly in the field of medicine and especially in infectious diseases. This, in turn, could lead to targeted interventions that reduce the spread of healthcare-associated pathogens. In this review, we begin with an introduction to the basics of ML. We then move on to discuss how ML can transform healthcare epidemiology, providing examples of successful applications. Finally, we present special considerations for those healthcare epidemiologists who want to use and apply ML\",\n",
        "    \"Recent advancements in Artificial Intelligence (AI) and Machine Learning (ML) technology have brought on substantial strides in predicting and identifying health emergencies, disease populations, and disease state and immune response, amongst a few. Although, skepticism remains regarding the practical application and interpretation of results from ML-based approaches in healthcare settings, the inclusion of these approaches is increasing at a rapid pace. Here we provide a brief overview of machine learning-based approaches and learning algorithms including supervised, unsupervised, and reinforcement learning along with examples. Second, we discuss the application of ML in several healthcare fields, including radiology, genetics, electronic health records, and neuroimaging. We also briefly discuss the risks and challenges of ML application to healthcare such as system privacy and ethical concerns and provide suggestions for future applications.\",\n",
        "    \"The drive towards greater penetration of machine learning in healthcare is being accompanied by increased calls for machine learning and AI based systems to be regulated and held accountable in healthcare. Interpretable machine learning models can be instrumental in holding machine learning systems accountable. Healthcare offers unique challenges for machine learning where the demands for explainability, model fidelity and performance in general are much higher as compared to most other domains. In this paper we review the notion of interpretability within the context of healthcare, the various nuances associated with it, challenges related to interpretability which are unique to healthcare and the future of interpretability in healthcare.\"\n",
        "]"
      ],
      "metadata": {
        "id": "4TgWQpgyAnNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Baseline Generation\n",
        "\n",
        "    . Use a fixed prompt (e.g., “Continue this paragraph in the style of …”). You can use a prompt like this:\n",
        "        GEN_PROMPT = (\n",
        "            \"Continue the following passage in the same style:\\n\\n\"\n",
        "            \"The results indicate a statistically significant improvement over baseline models under limited compute budgets.\"\n",
        "        )\n",
        "\n",
        "    . Generate continuations using the pretrained model without any fine‑tuning.\n",
        "    \n",
        "    Discussion on the steps take for tokenization and giving prompt:\n",
        "    1. Prompt Creation: i used a fixed prompt to guide the model to continue in the style of scientific abstracts, this ensure consistency in testing the generative ability of the pretrained model.\n",
        "    2. Tokenization: The prompt was converted into token id using the tokenizer. Tokenizer convert text to numberic ID that enable the model to process. The return_tensors=\"pt\" argument ensures the output is a PyTorch tensor, compatible with the model.\n",
        "    \n",
        "    3. Attention Mask & Pad Token Handling: GPT-2 model has no pad_token by default, so i set\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "    The attention mask is automatically created to tells the model which tokens are real input and which are padding. This prevents unexpected behavior during generation\n",
        "\n",
        "    \n",
        "    4. Text generation: i use the generate() method with parameters controlling randomness and repetition.\n",
        "    - input_ids: prompt converted into numbers (token IDs).\n",
        "    - attention_mask: tells the model which IDs are meaningful.\n",
        "    - max_new_tokens=80:  continuation token is limit to 80.\n",
        "    - do_sample=True: instead of always picking the single most likely next word, the model samples from a probability distribution.\n",
        "    - temperature=0.7: lowers randomness , text stays coherent.\n",
        "    - top_k=50: trims the candidate pool to top 50 likely words.\n",
        "    - top_p=0.95: nucleus sampling , only keep tokens whose combined probability more than 95%.\n",
        "    - repetition_penalty=1.2: discourages repeating the same phrases\n",
        "    - pad_token_id=tokenizer.eos_token_id: fixes GPT-2’s lack of pad token, avoids warnings.\n",
        "\n",
        "    5. Decoding\n",
        "    Finally token id was convert back to original readable text, provide a human-readable continuation of the prompt in the same style.\n",
        "\n"
      ],
      "metadata": {
        "id": "A1QSlvQDAT1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEN_PROMPT = (\n",
        "    \"Continue the following passage in the same style:\\n\\n\"\n",
        "    \"The results indicate a statistically significant improvement over baseline models under limited compute budgets.\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer(GEN_PROMPT, return_tensors=\"pt\").to(device)\n",
        "\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_new_tokens=80,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.2,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "end = time.time()\n",
        "\n",
        "# Decode to text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)\n",
        "print(f\"Inference time (P-task baseline): {end - start:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3GPvyPhPf8V",
        "outputId": "3667bdc3-76df-422b-c2b3-8cf8cc056d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continue the following passage in the same style:\n",
            "\n",
            "The results indicate a statistically significant improvement over baseline models under limited compute budgets. This is not surprising given that we have seen similar increases since 1980, and it seems unlikely at this time to be sustained for long term gains as well. As always, I would like to see these findings confirmed by additional studies of different types of computational tasks such (e-mailing or writing) with respect more frequently than those presented earlier.\n",
            "Inference time (P-task baseline): 1.24 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluation\n",
        "\n",
        "    . Qualitatively assess style similarity. (write a short analysis on what you see on generated data, in comparison with actual data, in terms of grammar and similarity)\n",
        "\n",
        "The generated text maintains a professional, scientific tone, consistent with the original prompt. Grammar and sentence structure are generally correct, and the model replicates the surface style of scientific writing by using phrases such as “statistically significant” and “baseline models.”\n",
        "\n",
        "Compared with the actual healthcare ML samples, the continuation shows some stylistic similarity in its formal, academic phrasing. However, unlike the domain samples, it lacks domain-specific vocabulary and instead introduces irrelevant examples (etc; “e-mailing or writing”) that would not typically appear in real abstracts. This highlights that the pretrained model captures the general style of scientific text but not the substance of the healthcare ML domain, which is expected since it was not fine-tuned on domain sample.\n",
        "\n",
        "Overall, the output represents a reasonable baseline for evaluation, demonstrating fluent academic style but limited domain grounding, which could be improved with fine-tuning.\n",
        "\n",
        "    . Search for a evaluation metric and compute this simple metric for the generated data (e.g., perplexity on held‑out domain samples, or BLEU/ROUGE comparing to original domain variants)."
      ],
      "metadata": {
        "id": "yEi48XZDMisk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity Function\n",
        "def compute_perplexity(model, tokenizer, text):\n",
        "    model.to(device)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "    perplexity = torch.exp(loss)\n",
        "    return perplexity.item()\n",
        "\n",
        "# Perplexity of Generated Text\n",
        "ppl_generated = compute_perplexity(model, tokenizer, generated_text)\n",
        "print(f\"Perplexity of generated text: {ppl_generated:.2f}\")\n",
        "\n",
        "\n",
        "# Perplexity of Domain Samples=\n",
        "import numpy as np\n",
        "\n",
        "domain_ppls = []\n",
        "for i, sample in enumerate(domain_sample, 1):\n",
        "    ppl = compute_perplexity(model, tokenizer, sample)\n",
        "    domain_ppls.append(ppl)\n",
        "    print(f\"Sample {i} Perplexity: {ppl:.2f}\")\n",
        "\n",
        "avg_domain_ppl = np.mean(domain_ppls)\n",
        "print(f\"\\nAverage Domain Perplexity: {avg_domain_ppl:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfWEcMuo3ikE",
        "outputId": "972edeba-b556-49d8-e2ae-de9055e39206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of generated text: 35.29\n",
            "Sample 1 Perplexity: 37.34\n",
            "Sample 2 Perplexity: 61.41\n",
            "Sample 3 Perplexity: 30.36\n",
            "Sample 4 Perplexity: 35.40\n",
            "Sample 5 Perplexity: 40.89\n",
            "\n",
            "Average Domain Perplexity: 41.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perplexity measures how surprised the model is by a given text. Lower values indicate the text is more predictable to the model, while higher values indicate greater difficulty. The generated text has a perplexity of 35.29, which is reasonable for a small GPT-2 model on scientific text it has not been fine-tuned on.\n",
        "\n",
        "Comparing with domain samples, the generated text’s perplexity is similar to Samples 1, 3, and 4 (30–40 range), suggesting a reasonable stylistic match. Sample 5 is moderately higher (40.89) but still close to the expected range. However, Sample 2 shows a much higher perplexity (61.41), indicating vocabulary or structure less familiar to DistilGPT-2, pointing to a domain mismatch.\n",
        "\n",
        "In short, the results are quite decent, since perplexity on in-domain text for GPT-2 typically falls in the 20–40 range, and most of our samples fall near this level. To further reduce perplexity and make the outputs more domain-natural, the next step would be fine-tuning GPT-2 on the target domain."
      ],
      "metadata": {
        "id": "DRXBTVh2z1vD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSCZIHouCLd-"
      },
      "source": [
        "\n",
        "## C-Level Tasks (~30%)\n",
        "\n",
        "Goal: Adaptation via Fine-Tuning\n",
        "\n",
        "1. Fine-tune the model (full-tuning) on your domain samples. Make choices (number of epochs, learning rate, context length, etc.) that fit within your computing environment.\n",
        "2. Generate same prompt continuation as the pass task.\n",
        "3. Re-evaluate the performance on sample generation. Compare style, fluency, and compare based on the evaluation metric you chose before.  \n",
        "4. Track the resources and report. Record the trainable parameters, training time, inference time.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "# Convert domain samples into Hugging Face Dataset\n",
        "dataset = Dataset.from_dict({\"text\": domain_sample})"
      ],
      "metadata": {
        "id": "33Htme_8keY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Fix pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "tokenized_ds = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "25ed4a40b54a4512a8866be628243c11",
            "96dda41f4abd4fed8d7e1a1c55aa8b90",
            "ffe7171c4369435e910f52464a659d08",
            "977a3fd33ba04f188d74ad486df72f03",
            "b7fd74a6e0c746e2ae55693d1fde064a",
            "2985e19fcde54138ac3dc57ae56f9dcb",
            "c7649a13ceb14f0c8856102f5588097f",
            "580d0f4f191f40c98d4b59092554682f",
            "400a8b3bfc9d42e6bde6faa827a17cb1",
            "4db4df8683604550aaf7908afc9fb6d3",
            "14fbec9a882b4e87a134ab614adc3175"
          ]
        },
        "id": "Fa2x8biAkqiH",
        "outputId": "35808093-05f7-4acb-e1c0-aa68651a864a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25ed4a40b54a4512a8866be628243c11"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training setup\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,             # few epochs (small dataset)\n",
        "    per_device_train_batch_size=2,  # small batch\n",
        "    learning_rate=5e-5,             # conservative LR\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=1,\n",
        "    report_to=\"none\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdkPvdpbkqrQ",
        "outputId": "e2594503-7a69-44a1-9e8e-4827ee72c2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2523752729.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train model\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "trainer.train()\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Training time: {end - start:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "f0IGqszxk4cC",
        "outputId": "a439e921-1f60-44f0-d5d7-45887b3a2f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9/9 00:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.642500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.647300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.158200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.494100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.174800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.029900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.398800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.276300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 1.55 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate with fine tune model\n",
        "inputs = tokenizer(GEN_PROMPT, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "start = time.time()\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=80,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.2,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "end = time.time()\n",
        "\n",
        "print(\"Fine-tuned continuation:\\n\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(f\"\\nInference time: {end - start:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1i5bVckk6m_",
        "outputId": "a4c2afa9-4d5c-427e-adfb-063611154495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned continuation:\n",
            "\n",
            "Continue the following passage in the same style:\n",
            "\n",
            "The results indicate a statistically significant improvement over baseline models under limited compute budgets. Our work will offer further evidence that we can improve our understanding of how to optimize for optimal performance across different datasets, such as general population and machine learning (GIS). We are now exploring data structures on functional languages using GIs with high-quality computational power or computing algorithms within C++ libraries which need more than adequate statistical analysis at best.[1] The findings demonstrate an important distinction between language processing\n",
            "\n",
            "Inference time: 0.94 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reevaluate the performance on sample generation.\n",
        "print(\"=== Perplexities After Fine-Tuning ===\")\n",
        "ppl_generated = compute_perplexity(model, tokenizer, GEN_PROMPT)\n",
        "print(f\"Generated text PPL: {ppl_generated:.2f}\")\n",
        "\n",
        "domain_ppls = []\n",
        "for i, sample in enumerate(domain_sample, 1):\n",
        "    ppl = compute_perplexity(model, tokenizer, sample)\n",
        "    domain_ppls.append(ppl)\n",
        "    print(f\"Sample {i} PPL: {ppl:.2f}\")\n",
        "\n",
        "avg_domain_ppl = np.mean(domain_ppls)\n",
        "print(f\"\\nAverage Domain Perplexity: {avg_domain_ppl:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Wxlg56lER5",
        "outputId": "b563fe0e-692e-466e-8d13-808b2d6a7f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Perplexities After Fine-Tuning ===\n",
            "Generated text PPL: 111.03\n",
            "Sample 1 PPL: 24.88\n",
            "Sample 2 PPL: 42.14\n",
            "Sample 3 PPL: 17.30\n",
            "Sample 4 PPL: 22.21\n",
            "Sample 5 PPL: 24.59\n",
            "\n",
            "Average Domain Perplexity: 26.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Introduce one modification of your own design (for example, a different hyperparameter setting, data sampling approach or model variant) and summarise its effect (performing the fine-tuning, sample generation, reevaluation and tracking the computational cost same as steps 1 to 4).\n"
      ],
      "metadata": {
        "id": "ZJm4keMdlnne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training setup with modified learning rate\n",
        "training_args_mod = TrainingArguments(\n",
        "    output_dir=\"./results_mod\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,             # keep same epochs\n",
        "    per_device_train_batch_size=2,\n",
        "    learning_rate=1e-4,             # MODIFIED learning rate\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=1,\n",
        "    report_to=\"none\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "trainer_mod = Trainer(\n",
        "    model=AutoModelForCausalLM.from_pretrained(model_id),  # reload fresh model\n",
        "    args=training_args_mod,\n",
        "    train_dataset=tokenized_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Fine-tune with modified setup\n",
        "import time\n",
        "start = time.time()\n",
        "trainer_mod.train()\n",
        "end = time.time()\n",
        "print(f\"Modified training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Generate with modified model\n",
        "inputs = tokenizer(GEN_PROMPT, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "start = time.time()\n",
        "outputs = trainer_mod.model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=80,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.2,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "end = time.time()\n",
        "\n",
        "print(\"\\nModified fine-tuned continuation:\\n\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(f\"\\nModified inference time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluate with perplexity\n",
        "print(\"\\n=== Perplexities After Modified Fine-Tuning ===\")\n",
        "ppl_generated = compute_perplexity(trainer_mod.model, tokenizer, GEN_PROMPT)\n",
        "print(f\"Generated text PPL: {ppl_generated:.2f}\")\n",
        "\n",
        "domain_ppls = []\n",
        "for i, sample in enumerate(domain_sample, 1):\n",
        "    ppl = compute_perplexity(trainer_mod.model, tokenizer, sample)\n",
        "    domain_ppls.append(ppl)\n",
        "    print(f\"Sample {i} PPL: {ppl:.2f}\")\n",
        "\n",
        "avg_domain_ppl = np.mean(domain_ppls)\n",
        "print(f\"\\nAverage Domain Perplexity: {avg_domain_ppl:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "wnD2C7_clos9",
        "outputId": "6a1b811e-1f06-4472-9b49-747f4645abe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3289619774.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_mod = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9/9 00:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.642500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.638800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.146000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.352600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.876100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.274100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.656500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.066200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.979000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified training time: 3.38 seconds\n",
            "\n",
            "Modified fine-tuned continuation:\n",
            "\n",
            "Continue the following passage in the same style:\n",
            "\n",
            "The results indicate a statistically significant improvement over baseline models under limited compute budgets. A large proportion of those who are not involved in this study do so because they have difficulty interpreting data, which may lead to errors and inaccuracies for general populations (eg., diabetes). However it is important that statistical analyses can be used as an exploratory tool where we discuss such issues with our patients before undertaking clinical trials or at-risk studies based on these findings; otherwise information about them will likely\n",
            "\n",
            "Modified inference time: 1.16 seconds\n",
            "\n",
            "=== Perplexities After Modified Fine-Tuning ===\n",
            "Generated text PPL: 111.52\n",
            "Sample 1 PPL: 19.01\n",
            "Sample 2 PPL: 28.76\n",
            "Sample 3 PPL: 11.36\n",
            "Sample 4 PPL: 15.38\n",
            "Sample 5 PPL: 15.84\n",
            "\n",
            "Average Domain Perplexity: 18.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Discuss the trade-off between computational cost and sample generation performance based on the mothods you performed in this task.\n",
        "\n",
        "| Method                 | Trainable Params | Training Time (s) | Inference Time (s) | Generated PPL | Domain PPL (avg) | Qualitative Description       |\n",
        "| ---------------------- | ---------------- | ----------------- | ------------------ | ------------- | ---------------- | ----------------------------- |\n",
        "| **Baseline (P-task)**  | 0                | 0                 | 1.24             | 35.29     | 41.08        | Fluent, generic               |\n",
        "| **Full FT (5e-5)**     | 82M            | 1.55             | 0.94              | 111.03        | 26.22            | Domain-ish (somewhat related to the domain, but not exactly or fully aligned), incoherent        |\n",
        "| **Modified FT (1e-4)** | 82M            | 3.38              | 1.16              | 111.52        | 18.07    | Domain-aligned, more coherent |\n",
        "\n",
        "\n",
        "1. fine-tuning procedure and rationale  \n",
        "The five short healthcare on machine learning domain sample are fine tuned with DistilGPT-2 (82 paramater). Fine tuned was chosen to evaluate whether the model could adapt to the target domain and surpass the performance of the baseline. Training used\n",
        "- epoch(3): kept low because the dataset is tiny (only 5 domainsample), more epochs would likely overfit.\n",
        "- batch size (2): small to reduce memory use and ensure more gradient update\n",
        "- learning rate (5e-5, then increased to 1e-4 for the modification): 5e-5 provide stable update, while 1e-4 allow the model to learn more, tested to accelerate adaption\n",
        "- max sequence length (256): chosen to cover the longest paragraph .\n",
        "\n",
        "The rationale was to test domain allignment changes under the same small data conditions and different learning rate.\n",
        "\n",
        "\n",
        "2. regenerated prompt comparison\n",
        "- Baseline (P-task): Fluent and grammatically correct, but generic and off-topic (etc; e-mailing or writing).\n",
        "- Fine tuned (lr=5e-5): Introduced technical jargon and domain terms (machine learning, functional languages, C++ libraries) but with less coherence and healthcare focus. the output was more technical but less readable.\n",
        "- Modified fine tuned (lr=1e-4): More coherent and domain alligned (healthcare focus; mentions diabetes, patients, statistical analyses, clinical trials), but still less fluent than baseline.\n",
        "\n",
        "This shows a trade off, fine tuning improve domain specificity but at the cost of fluency, especially with very small dataset (only 5 domain samples).\n",
        "\n",
        "\n",
        "3. computational resource comparison\n",
        "- Baseline: no training needed, inference is 1.24 s.\n",
        "- Fine-tuned (5e-5): training took 1.55s, inference 0.94s. Generated PPL worsened, but domain PPL improved.  \n",
        "- Modified fine-tuned (1e-4):training took 3.38s, inference 1.16s. Domain PPL improved further, but generated PPL remained high.  \n",
        "\n",
        "Fine tuning require more computation but produced lower domain perplexity. However, this came at the cost of higher generated perplexity and reduced fluency on unseen prompts. In other words, the model become more domain specialized but less generalizable, indicating overfitting when training on very few samples, hence D task will focus on PEFT to reduce compute cost and avoid overfitting.\n"
      ],
      "metadata": {
        "id": "UVOYx4_mwSCI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFcWSIbDCiQ9"
      },
      "source": [
        "## D-Level Tasks (~10%)\n",
        "\n",
        "Goal: Parameter-Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "\n",
        "1. Apply a parameter-efficient adaptation method (for example LoRA or adapter modules) to your model. Discuss the justification behind choosing an specific form of PEFT.\n",
        "\n",
        "LoRA introduces low-rank adapter matrices into transformer attention layers while freezing the majority of pretrained weights. This enables the model to preserve general fluency while adapting to a new domain and have lower risk of overfitting.  \n",
        "\n",
        "2. Apply chosen method using your domain data. Generate same prompt continuation. Re-evaluate performance on sample generation. Compare style, fluency, and compare based on the evaluation metric you chose before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Reload base model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Define LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,                 # rank of LoRA matrices (trade-off between cost/performance)\n",
        "    lora_alpha=32,\n",
        "    bias=\"all\"\n",
        ")\n",
        "\n",
        "# Wrap model with LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Count trainable params\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOe_iKAhlpuc",
        "outputId": "4a9009ea-e572-4f68-952b-f047608a9490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 198,912 || all params: 82,060,032 || trainable%: 0.2424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args_lora = TrainingArguments(\n",
        "    output_dir=\"./results_lora\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,             # keep same epochs\n",
        "    per_device_train_batch_size=2,\n",
        "    learning_rate=5e-4,             # MODIFIED learning rate\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=1,\n",
        "    report_to=\"none\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "trainer_lora = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_lora,\n",
        "    train_dataset=tokenized_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "trainer_lora.train()\n",
        "end = time.time()\n",
        "print(f\"LoRA training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Generate continuation\n",
        "inputs = tokenizer(GEN_PROMPT, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "start = time.time()\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=80,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.2,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "end = time.time()\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\nLoRA fine-tuned continuation:\\n\")\n",
        "print(generated_text)\n",
        "print(f\"\\nLoRA inference time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluate perplexity\n",
        "print(\"\\n=== Perplexities After LoRA Fine-Tuning ===\")\n",
        "print(f\"Generated text PPL: {compute_perplexity(model, tokenizer, GEN_PROMPT):.2f}\")\n",
        "\n",
        "domain_ppls = []\n",
        "for i, sample in enumerate(domain_sample, 1):\n",
        "    ppl = compute_perplexity(model, tokenizer, sample)\n",
        "    domain_ppls.append(ppl)\n",
        "    print(f\"Sample {i} PPL: {ppl:.2f}\")\n",
        "\n",
        "avg_domain_ppl = sum(domain_ppls) / len(domain_ppls)\n",
        "print(f\"\\nAverage Domain PPL: {avg_domain_ppl:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "HocoeCg8xD1x",
        "outputId": "756617d0-3888-425a-e499-d25dcb099f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3380451781.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_lora = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9/9 00:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.642500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.673200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.169900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.656100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.521600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.654300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.447200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.719200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.603900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA training time: 1.14 seconds\n",
            "\n",
            "LoRA fine-tuned continuation:\n",
            "\n",
            "Continue the following passage in the same style:\n",
            "\n",
            "The results indicate a statistically significant improvement over baseline models under limited compute budgets. Our regression analysis is based on four-point performance and comparison of these two groups, with respect to overall model size (mean for both cases). All data are represented as follows; Table 1 shows an estimated mean trend from one year thereafter within each group by linear fit across all covariates throughout our test suite including three main effects that account only for statistical significance among those whose variance represents nonparametric changes\n",
            "\n",
            "LoRA inference time: 0.92 seconds\n",
            "\n",
            "=== Perplexities After LoRA Fine-Tuning ===\n",
            "Generated text PPL: 110.06\n",
            "Sample 1 PPL: 34.49\n",
            "Sample 2 PPL: 60.07\n",
            "Sample 3 PPL: 27.63\n",
            "Sample 4 PPL: 32.04\n",
            "Sample 5 PPL: 38.23\n",
            "\n",
            "Average Domain PPL: 38.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Compare resource use (trainable parameter count, training time) and performance against the full fine-tuning methods mentioned on the C task.  \n",
        "\n",
        "| Method                 | Trainable Params | Training Time (s) | Inference Time (s) | Generated PPL | Domain PPL (avg) | Qualitative Description                |\n",
        "| ---------------------- | ---------------- | ----------------- | ------------------ | ------------- | ---------------- | -------------------------------------- |\n",
        "| **Baseline (P-task)**  | 0                | 0                 | 1.24               | 35.29         | 41.08            | Fluent, generic                        |\n",
        "| **Full FT (5e-5)**     | 82M              | 1.55              | 0.94               | 111.03        | 26.22            | Domain-ish, incoherent, overfit        |\n",
        "| **Modified FT (1e-4)** | 82M              | 3.38              | 1.16               | 111.52        | 18.47            | Domain-aligned but overfit |\n",
        "| **LoRA (r=8, α=32 )**        | 0.16M (0.2%)    | 1.14              | 0.92               | 110.06        | 38.49            | Fluent, structured, moderate domain focus  |\n",
        "\n",
        "LORA significantly reduce the number of trainable parameters. While fine tune updated all 82M parameter, LORA update only 0.16M(0.2%), making it far more lightweight. Training time was shorter for LORA as it consumed much less memory which would matter at scale, Inference was also slightly faster for LORA compared to the fine tuned one.\n",
        "\n",
        "Performance-wise, modified FT achieved much lower domain perplexity (better adaption) but at the cost of very high generated perplexity (loss fluency). On the other hand, LORA achieve the best generated perplexity (110.06), indicate more structured generalization, but highest average domain perplexity, indicate weaker domain alignment but less overfitting.\n",
        "\n",
        "4. Discuss the trade-off between computational cost and sample generation performance in comparison with previous methods in task C.\n",
        "Key points to cover:\n",
        "\n",
        "a) Parameter-Efficient Fine-Tuning procedure and rationale  \n",
        "\n",
        "LORA inserted small low rank adapter matrices in attention layers, leaving most pretrained weight frozen. This mean the model preserves general fluency while advancing towards the target domain. It is specially suitable for small dataset and limited compute as its training is memory and time efficient and also avoid updating all model weight eventually decrease overfitting.\n",
        "\n",
        "config used:\n",
        "- small r=8 keep the update very lightweight\n",
        "- α = 32 strengthen the adapter updates so they still have effect.\n",
        "- bias=\"all\", slightly increase capacity to help compensate for the tiny dataset.\n",
        "- learning rate set to 5e-4 since LORA has far fewer trainable paramater, a higher learning rate helps the small adapter learn meaningful update quickly.\n",
        "\n",
        "The rationale of LORA is designed to mitigate overfitting and reduce compute when adapting large scale real world large language models to small dataset.\n",
        "\n",
        "\n",
        "b) regenerated prompt comparison  \n",
        "\n",
        "- Fine tuned (lr=5e-5): Introduced technical jargon and domain terms (machine learning, functional languages, C++ libraries) but with less coherence and healthcare focus. the output was more technical but less readable.\n",
        "- Modified fine tuned (lr=1e-4): More coherent and domain alligned (healthcare focus; mentions diabetes, patients, statistical analyses, clinical trials), but still less fluent than baseline (clear overfitting).\n",
        "- LoRA: Produced a fluent, structured academic continuation with statistical terms (regression analysis, statistical trend). However, it was less healthcare-specific than the Modified fine tuned, reflecting weaker domain alignment but stronger preservation of general fluency.\n",
        "\n",
        "Trade-off:\n",
        "\n",
        "Fine Tuned = strong domain fit but incoherent, overfit text.\n",
        "\n",
        "LoRA = weaker domain fit but better fluency and structure.\n",
        "\n",
        "c) computational resource comparison\n",
        "\n",
        "Fine tuned require updating all 82M parameter, resulting in higher memory use, longer training, and a greater risk of overfitting on small datasets. In contrast, LORA updates only 0.2% of parameters, making training much faster and lighter on compute, while inference also remains efficient. Although LORA achieved weaker domain adaptation (higher domain perplexity), it preserved fluency and structure better than fine-tuning. With only five domain samples, the fine-tuned models tend to memorize and overfit, whereas LORA maintained generalization despite adapting only partially.\n",
        "\n",
        "With larger domain datasets, LoRA would be expected to reduce domain perplexity significantly while still retaining fluency, whereas fine-tuning would continue to risk overfitting and demand far higher computational cost (Wang & Li, 2023). That is why real world fine tuning almost always use PEFT such as LORA because it is cheaper, avoid overfitting, and still achieve strong domain adaption when sufficient data is available. Furthermore, hyperparameter can be tuned until performance benchmark are met.  "
      ],
      "metadata": {
        "id": "fPozS4EKoT1i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmxq_M0rO6PQ"
      },
      "source": [
        "## References:\n",
        "\n",
        "1. Wang, F., & Li, B. (2025). Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA. arXiv preprint arXiv:2506.20856.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "25ed4a40b54a4512a8866be628243c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96dda41f4abd4fed8d7e1a1c55aa8b90",
              "IPY_MODEL_ffe7171c4369435e910f52464a659d08",
              "IPY_MODEL_977a3fd33ba04f188d74ad486df72f03"
            ],
            "layout": "IPY_MODEL_b7fd74a6e0c746e2ae55693d1fde064a"
          }
        },
        "96dda41f4abd4fed8d7e1a1c55aa8b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2985e19fcde54138ac3dc57ae56f9dcb",
            "placeholder": "​",
            "style": "IPY_MODEL_c7649a13ceb14f0c8856102f5588097f",
            "value": "Map: 100%"
          }
        },
        "ffe7171c4369435e910f52464a659d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_580d0f4f191f40c98d4b59092554682f",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_400a8b3bfc9d42e6bde6faa827a17cb1",
            "value": 5
          }
        },
        "977a3fd33ba04f188d74ad486df72f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4db4df8683604550aaf7908afc9fb6d3",
            "placeholder": "​",
            "style": "IPY_MODEL_14fbec9a882b4e87a134ab614adc3175",
            "value": " 5/5 [00:00&lt;00:00, 122.91 examples/s]"
          }
        },
        "b7fd74a6e0c746e2ae55693d1fde064a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2985e19fcde54138ac3dc57ae56f9dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7649a13ceb14f0c8856102f5588097f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "580d0f4f191f40c98d4b59092554682f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "400a8b3bfc9d42e6bde6faa827a17cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4db4df8683604550aaf7908afc9fb6d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14fbec9a882b4e87a134ab614adc3175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}